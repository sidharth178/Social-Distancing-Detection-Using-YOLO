{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:green\" align=\"center\"><b>Social Distancing Detection</b> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "from utils import transparentOverlay1, dst_circle, get_bounding_box, int_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS\n",
    "WRITE_VIDEO = True\n",
    "SHOW_OUTPUT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_source):\n",
    "\n",
    "    print(\"[INFO] loading YOLO ...\")\n",
    "    net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "    \n",
    "    # determine only the *output* layer names that we need from YOLO\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    print(\"[INFO] start video streaming ...\")\n",
    "    # initialize the video stream\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "    if WRITE_VIDEO:\n",
    "        # output video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (1080, 720))\n",
    "\n",
    "        \n",
    "    i = 0\n",
    "    # loop over the frames from the video stream\n",
    "    while (cap.isOpened()):\n",
    "        \n",
    "        # read the next frame from the file\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # If the frame was not grabbed,then we have reached the end of the stream\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        i += 1\n",
    "        if not (i % 3 == 0): continue\n",
    "        # grab the dimensions of the frame\n",
    "        height, width, channels = frame.shape\n",
    "        \n",
    "        # construct a blob from the input frame and then perform a forward\n",
    "        # pass of the YOLO object detector, giving us our bounding boxes\n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.00392, (608, 608), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "        boxes, confidences, class_id = get_bounding_box(outs, height, width)\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "       \n",
    "        circles = []\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]                                \n",
    "                cx, cy = x + w // 2, y + h\n",
    "                # draw a vertical line from the center of circle to center of boundary box \n",
    "                frame = cv2.line(frame, (cx, cy), (cx, cy - h // 2), (0, 255, 0), 2)\n",
    "                # draw circle around the people\n",
    "                frame = cv2.circle(frame, (cx, cy - h // 2), 5, (255, 20, 200), -1)\n",
    "                circles.append([cx, cy - h // 2, h])\n",
    "\n",
    "        int_circles_list = []\n",
    "        indexes = []\n",
    "        for i in range(len(circles)):\n",
    "            x1, y1, r1 = circles[i]\n",
    "            for j in range(i + 1, len(circles)):\n",
    "                x2, y2, r2 = circles[j]\n",
    "                if int_circle(x1, y1, x2, y2, r1 // 2, r2 // 2) >= 0 and abs(y1 - y2) < r1 // 4:\n",
    "                    indexes.append(i)\n",
    "                    indexes.append(j)\n",
    "\n",
    "                    int_circles_list.append([x1, y1, r1])\n",
    "                    int_circles_list.append([x2, y2, r2])\n",
    "                    cv2.line(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "            rows, cols, _ = frame.shape\n",
    "            for i in range(len(circles)):\n",
    "                x, y, r = circles[i]\n",
    "\n",
    "                if i in indexes:\n",
    "                    color = (0, 0, 255)\n",
    "                else:\n",
    "                    color = (0, 200, 20)\n",
    "                scale = (r) / 100\n",
    "                \n",
    "                transparentOverlay1(frame, dst_circle, (x, y - 5), alphaVal=110, color=color, scale=scale)\n",
    "                cv2.rectangle(frame, (0, rows - 80), (cols, rows), (0, 0, 0), -1)\n",
    "                \n",
    "             # It'll show how many people are there in that place   \n",
    "            cv2.putText(frame,\n",
    "                        \"Total Persons : \" + str(len(boxes)),\n",
    "                        (25, 800),\n",
    "                        fontFace=cv2.QT_FONT_NORMAL,\n",
    "                        fontScale=1,\n",
    "                        color=(0, 0, 255))\n",
    "            \n",
    "            # It'll show how many people violate the social distancing rule\n",
    "            cv2.putText(frame,\n",
    "                        \"Defaulters : \" + str(len(set(indexes))),\n",
    "                        (425, 800),\n",
    "                        fontFace=cv2.QT_FONT_NORMAL,\n",
    "                        fontScale=1,\n",
    "                        color=(0, 0, 255))\n",
    "            \n",
    "        # save the detection footage    \n",
    "        if WRITE_VIDEO:\n",
    "            out.write(frame)\n",
    "        #  PRESS 'q' buttom to end the detection\n",
    "        if SHOW_OUTPUT:\n",
    "            cv2.imshow(\"frame\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    if WRITE_VIDEO:\n",
    "        out.release()\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO ...\n",
      "[INFO] start video streaming ...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # path to input video\n",
    "    video = \"pedestrians.mp4\"\n",
    "    if len(sys.argv) > 1:\n",
    "        video_source = sys.argv[1]\n",
    "    else:\n",
    "        video_source = \"pedestrians.mp4\"\n",
    "\n",
    "    main(video_source=video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
